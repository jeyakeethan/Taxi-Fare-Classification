{"cells":[{"metadata":{},"cell_type":"markdown","source":"Import Required modules","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# import requried modules\n\nimport numpy as np\nimport pandas as pd\n\nfrom xgboost import XGBClassifier\n\nfrom matplotlib import pyplot\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import metrics\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multioutput import MultiOutputClassifier\n\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import roc_curve, roc_auc_score, classification_report, confusion_matrix, f1_score\n\nfrom numpy import mean\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom collections import Counter\n\nfrom numpy import loadtxt\nfrom numpy import sort\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import SelectFromModel\n\nfrom collections import OrderedDict\nfrom itertools import product\nimport random\nimport os\nfrom importlib import reload\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"RANDOM_SEED = 1    # Set a random seed for reproducibility!\n\ntaxidata = pd.read_csv('../input/newdata/train_new.csv', nrows = 100_000, index_col='tripid')\ntestdata = pd.read_csv('../input/newdata/test_new.csv', nrows = 100_000, index_col='tripid')\n\ntaxidata.isnull().sum(axis = 0)\ntaxidata=taxidata.dropna(how='any',axis=0)\n# taxidata=taxidata.dropna(how='any',axis=0, subset=['fare'])\n# taxidata=taxidata.fillna(value={'fare': 0, 'additional_fare':0,\t'duration':0,\t'meter_waiting':0, 'meter_waiting_fare':0,\t'meter_waiting_till_pickup':0})\n\"\"\"Convert correct incorrect strings into Boolean values\"\"\"\nmat_t = {'correct': 1, 'incorrect': 0}\ntaxidata['label'] = taxidata['label'].replace(mat_t)\n\n# calculate distances\n# testdata[\"distance\"] = np.sqrt(np.sum(np.square([testdata[\"pick_lat\"]-testdata[\"drop_lat\"], testdata[\"pick_lon\"]-testdata[\"drop_lon\"]]), axis=0))*10000\n\ntaxidata.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature Importance Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# define custom class to fix bug in xgboost 1.0.2\nclass MyXGBClassifier(XGBClassifier):\n    @property\n    def coef_(self):\n        return None\n\n#Drop objective coumns\nselected_features = [feat for feat in taxidata.columns if feat not in ['pickup_time', 'drop_time', 'pick_lat', 'pick_lon', 'drop_lat', 'drop_lon']]\ntaxidata = taxidata[selected_features]\nX = taxidata.iloc[:,:-1]\nY = taxidata.iloc[:,-1:]\n\nX['waiting_ratio'] = X['meter_waiting']/(X['meter_waiting_fare']+1)\nselected_features.append('waiting_ratio')\n\n# split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=RANDOM_SEED)\nX_train.describe()\n# fit model on all training data\nmodel = MyXGBClassifier()\nmodel.fit(X_train, y_train)\nplot_importance(model)\npyplot.show()\n# make predictions for test data and evaluate\npredictions = model.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n# Fit model using each importance as a threshold\nthresholds = sort(model.feature_importances_)\nfor thresh in thresholds:\n    # select features using threshold\n    selection = SelectFromModel(model, threshold=thresh, prefit=True)\n    select_X_train = selection.transform(X_train)\n    # train model\n    selection_model = XGBClassifier()\n    selection_model.fit(select_X_train, y_train)\n    # eval model\n    select_X_test = selection.transform(X_test)\n    predictions = selection_model.predict(select_X_test)\n    accuracy = accuracy_score(y_test, predictions)\n    print(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Smote Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop objective coumns\nselected_features = [feat for feat in taxidata.columns if feat not in ['pickup_time', 'drop_time', 'pick_lat', 'pick_lon', 'drop_lat', 'drop_lon']]\ntaxidata = taxidata[selected_features]\n\nX = taxidata.iloc[:,:-1]\nY = taxidata.iloc[:,[-1]].values[:,0]\n\nX_train, X_eval, y_train, y_eval = train_test_split(\n    X,\n    Y,\n    test_size=0.33,\n    shuffle=True,\n    stratify=labels_df,\n    random_state=RANDOM_SEED\n)\n\nnumeric_preprocessing_steps = Pipeline([\n    ('standard_scaler', StandardScaler()),\n    ('simple_imputer', SimpleImputer(strategy='median'))\n])\npreprocessor = ColumnTransformer(\n    transformers = [\n        (\"numeric\", numeric_preprocessing_steps, categorical),\n    ],\n    remainder = \"drop\"\n)\n\nmodel = estimators = estimator=XGBClassifier(\n                    objective='binary:logistic',\n                    booster='gbtree',   #default from gbtree, gblinear or dart\n                    learning_rate =0.05,   \n                    n_estimators=1500,\n                    max_depth=4,    #default 6, increase will overfit the model\n                    min_child_weight=6,\n                    gamma=1,\n                    subsample=0.8,\n                    colsample_bytree=0.8,\n                    reg_alpha=0.005,    #default 1\n                    reg_lambda=0,   #default 0\n                    scale_pos_weight=1,\n)\nfull_pipeline = Pipeline([\n    (\"preprocessor\", preprocessor),\n    (\"estimators\", estimators),\n])\n# values to evaluate\nk_values = [1,2,3,4,5,6,7]\nfor k in k_values:\n    over = SMOTE(sampling_strategy=0.1, k_neighbors=k)\n    under = RandomUnderSampler(sampling_strategy=0.5)\n    steps = [('over', over), \n#              ('under', under),\n             ('model', model)]\n    pipeline = Pipeline(steps=steps)\n    # evaluate pipeline\n    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n    print(cv.split(X, y))\n    scores = cross_val_score(pipeline, X, y, scoring='f1_macro', cv=cv, n_jobs=-1)\n    print(scores)\n    score = mean(scores)\n    print('> k=%d, Mean f1_macro: %.3f' % (k, score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Select features based on importance & Remove Noisy Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# taxidata_noise = taxidata[\\\n# #                           (taxidata.fare<taxidata.additional_fare+taxidata.meter_waiting_fare)\\\n# #                           |(taxidata.duration<taxidata.meter_waiting)\\\n#                           (taxidata.duration>21.5)\\   #21.5 min value for wrong label\n#                           |(taxidata.additional_fare>=5)\\   #5 min value for wrong label\n#                           |(taxidata.meter_waiting<11723)\\   #max value for wrong label\n# #                           |((taxidata.distance==0)&(taxidata.meter_waiting==0))|\\\n#                            |(taxidata.fare>60)]    #60 min fare for wrong label\n# taxidata = taxidata[(~taxidata.index.isin(taxidata_noise.index))]\n\n# for col in taxidata.:\n#     taxidata = taxidata[taxidata[col].between(taxidata1[col].quantile(0.01), taxidata1[col].quantile(0.99))]\n\n# taxidata = taxidata[(taxidata.fare>taxidata.additional_fare+taxidata.meter_waiting_fare)| (taxidata.label==False)]\n# taxidata = taxidata[(taxidata.fare!=0)| (taxidata.label==False)]\n# taxidata = taxidata[taxidata.additional_fare==10.5]\n# taxidata = taxidata[taxidata.meter_waiting_fare<105]\n# taxidata = taxidata[taxidata.meter_waiting_fare<140]\n# taxidata = taxidata[taxidata.meter_waiting_till_pickup<510]\n# taxidata = taxidata[taxidata.meter_waiting<2400]\n# taxidata = taxidata[taxidata.duration<6500]\n# taxidata = taxidata[taxidata.duration.between(taxidata.duration.quantile(.01),taxidata.duration.quantile(.99))]\n# taxidata = taxidata[(taxidata.fare!=0)| (taxidata.label==False)]\n# taxidata = taxidata[taxidata.fare<2000]\n# taxidata = taxidata[(taxidata.meter_waiting_till_pickup<1510)]\n# taxidata = taxidata[(taxidata.meter_waiting<14000)]\n# taxidata = taxidata[(taxidata.duration<16500)]\n# taxidata = taxidata[(taxidata.fare<10000)]\n\n\n# taxidata = taxidata[(taxidata.meter_waiting_fare<1005)]\n# taxidata = taxidata[(taxidata.meter_waiting_till_pickup<1510)]\n# taxidata = taxidata[(taxidata.meter_waiting<14000)]\n# taxidata = taxidata[(taxidata.duration<16500)]\n# taxidata = taxidata[(taxidata.fare<10000)]\n# taxidata.fare = taxidata.fare - taxidata.additional_fare\n# taxidata = taxidata.drop('additional_fare', axis=1)\n# taxidata = taxidata[taxidata.fare>taxidata.additional_fare]\n\n\n# minmax = {}\n# for index in taxidata.columns:\n#     if index != 'label':\n#         min0 = taxidata[taxidata.label==0][index].min()\n#         max0 = taxidata[taxidata.label==0][index].max()\n#         min1 = taxidata[taxidata.label==1][index].min()\n#         max1 = taxidata[taxidata.label==1][index].max()\n#         minmax[index] = (min0 if min0>min1 else min1, max0 if max0<max1 else max1)\n# #         minmax[index] = (min1, max1)\n#         print(minmax[index])\n#         taxidata = taxidata[(minmax[index][0]<taxidata[index])&(taxidata[index]<minmax[index][1])]\n# taxidata.describe()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop objective coumns\n\nselected_features = [feat for feat in taxidata.columns if feat not in ['pickup_time', 'drop_time', 'pick_lat', 'pick_lon', 'drop_lat', 'drop_lon']]\ntaxidata = taxidata[selected_features]\n\ntaxidata['waiting_ratio'] = taxidata['meter_waiting']/(taxidata['meter_waiting_fare']+1)\n\n#Drop least influence features\nselected_features = [feat for feat in taxidata.columns if feat not in ['meter_waiting_fare', 'additional_fare',]] # 'duration_t']]\ntaxidata = taxidata[selected_features]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hyper Param Tuning Functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyper_param_tuning import *\nX = taxidata.iloc[:,:-1]\nY = taxidata.iloc[:,[-1]]\n# split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n\ndata0 = { \"x_train\" : X_train,\n         \"y_train\" : y_train,\n         \"x_test\"  : X_test,\n         \"y_test\"  : y_test}\n\n#%%\nparam_grid_dic = OrderedDict([\n        (\"learning_rate\", [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ),\n        (\"max_depth\"    , [  3 , 4 , 5, 6,  8, 10, 12, 15 ] ),\n        (\"min_child_weight\", [ 1, 3, 5, 7 ] ),\n        (\"gamma\", [0.0, 0.1, 0.2, 0.3, 0.4 ]),\n        (\"colsample_bytree\", [  0.3, 0.4, 0.5, 0.7 ] ),\n        ])\n\n#%%\ntrain_fraction = 0.05\ntest_fraction = 0.16\ndata = subsample( data0, train_fraction, test_fraction )\n#%%\nseed=1359\nlog_level=0\n#%%\nmemoization_path = \"xgboost_memo%g\" % train_fraction\nprint( \"memoization_path= \" + memoization_path)\nif not os.path.exists( memoization_path ) :\n    os.mkdir( memoization_path )\n#%%\nfun = Memoizer( lambda param_dic : train_xgb( data, param_dic),\n                memoization_path )\n#%%\ngrid_search( param_grid_dic, fun , seed)\n#%%","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocess Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = taxidata.iloc[:,:-2]\nX['waiting_ratio'] = taxidata['waiting_ratio']\nY = taxidata.iloc[:,-2:-1]\n\n# split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n\nnp.testing.assert_array_equal(X.index.values, Y.index.values)\n\nnull_columns=X.columns[X.isnull().any()]\nprint(\"columns contains null data\", X[null_columns].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fit Model and Evaluate","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# chain preprocessing into a Pipeline object\n# each step is a tuple of (name you chose, sklearn transformer)\nnumeric_preprocessing_steps = Pipeline([\n    ('standard_scaler', StandardScaler()),\n    ('simple_imputer', SimpleImputer(strategy='median'))\n])\n\nif 'label' in selected_features: selected_features.remove('label')\npreprocessor = ColumnTransformer(\n    transformers = [\n        (\"numeric\", numeric_preprocessing_steps, selected_features),\n    ],\n    remainder = \"drop\"\n)\n\ntaxidata.describe()\n\nfull_pipeline = Pipeline([\n    (\"preprocessor\", preprocessor),\n])\n\nX_train = full_pipeline.fit(X_train)\n\n# over = SMOTE(sampling_strategy=0.5, k_neighbors=7)\n# print(Counter(Y.label))\n# X, Y = over.fit_resample(X, Y)\n# print(Counter(Y.label))\nX_train, X_eval, y_train, y_eval = train_test_split(\n    X,\n    Y,\n    test_size=0.33,\n    shuffle=True,\n    stratify=Y,\n#     random_state=RANDOM_SEED\n)\n\nX_train.shape,y_train.shape\n\n# over = SMOTE(sampling_strategy=0.5, k_neighbors=7)\n# print(Counter(y_train.label))\n# X_train, y_train = over.fit_resample(X_train, y_train)\n# print(Counter(y_train.label))\n\n# under = RandomUnderSampler(sampling_strategy=0.1)\n# print(Counter(y_train.label))\n# X_train, y_train = under.fit_resample(X_train, y_train)\n# print(Counter(y_train.label))\n\ndef f1_eval(y_pred, dtrain):\n    y_true = dtrain.get_label()\n    err = 1-f1_score(y_true, np.round(y_pred))\n    return 'f1_err', err\n\n# estimate scale_pos_weight value\ncounter = Counter(Y.label)\nest_scale_pos_weight = counter[0] / counter[1]\n\neval_set = [(X_train, y_train), (X_eval, y_eval)]\n\n# model = MultiOutputClassifier(estimator = \nmodel = XGBClassifier(\n        booster='gbtree',   #default from gbtree and gblinear or dart\n        learning_rate =0.05,   \n        n_estimators=2500,\n        max_depth=3,    #default 6, increase will overfit the model [  3 , 4 , 5, 6,  8, 10, 12, 15 ]\n        min_child_weight=6,    # [ 1, 3, 5, 7 ] \n        gamma=0.2,\n        subsample=1,\n        colsample_bytree=1,    # [  0.3, 0.4, 0.5, 0.7 ]\n        reg_alpha=0.005,\n        reg_lambda=0.09,   #default 0 for \"majority class set to 0\"\n        objective='binary:logistic',\n        scale_pos_weight= 0.35,\n        sampling_method = 'gradient_based',\n#         sample_type='weighted',    #for dart only\n#         rate_drop = 0.1,    #for dart only\n#         seed=27\n    verbosity = 0,\n    )\n# )\n\nmodel.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=True)\n\npredictions = model.predict(X_eval)\n\n# evaluate predictions\naccuracy = accuracy_score(y_eval, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n\nprint(model.score(X_eval,y_eval))\nprint(confusion_matrix(y_eval,predictions))\nprint(classification_report(y_eval,predictions))\n\n# retrieve performance metrics\nresults = model.evals_result()\nepochs = len(results['validation_0']['error'])\nx_axis = range(0, epochs)\n# plot log loss\nfig, ax = pyplot.subplots()\nax.plot(x_axis, results['validation_0']['logloss'], label='Train')\nax.plot(x_axis, results['validation_1']['logloss'], label='Test')\nax.legend()\npyplot.ylabel('Log Loss')\npyplot.title('XGBoost Log Loss')\npyplot.show()\n# plot classification error\nfig, ax = pyplot.subplots()\nax.plot(x_axis, results['validation_0']['error'], label='Train')\nax.plot(x_axis, results['validation_1']['error'], label='Test')\nax.legend()\npyplot.ylabel('Classification Error')\npyplot.title('XGBoost Classification Error')\npyplot.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predict Unlabeled Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"null_columns=testdata.columns[testdata.isnull().any()]\ntestdata[null_columns].isnull().sum()\n\ntestdatadup = testdata[:]\n\ntestdatadup['waiting_ratio'] = testdatadup['meter_waiting']/(testdatadup['meter_waiting_fare']+1)\n\ntestdata0 = testdatadup[(testdatadup.fare<testdatadup.additional_fare+testdatadup.meter_waiting_fare)|(testdatadup.duration<testdatadup.meter_waiting)|(testdatadup.duration==0)|((testdatadup.distance==0)&(testdatadup.meter_waiting==0))|(testdatadup.fare==0)]\ntestdata1 = testdatadup[(~testdatadup.index.isin(testdata0.index))]\ntestdata0.shape\n\nif 'label' in selected_features: selected_features.remove('label')\ntestdata1 = testdata1[selected_features]\ntestdata0 = testdata0[selected_features]\ntestdata0['prediction'] = 0\ntest_probas = model.predict(testdata1)\n\n# Save predictions to submission data frame\ntestdata1[\"prediction\"] = test_probas\n# print(testdata0.describe())\n# print(testdata1.describe())\npredicted_df=testdata1.append(testdata0)\n# print(predicted_df.describe())\nsubmission_df = pd.read_csv('../input/sample/sample_submission.csv',index_col='tripid')\npredicted_df = predicted_df.reindex(submission_df.index)\n\n\n# Make sure we have the rows in the same order\nnp.testing.assert_array_equal(predicted_df.index.values, submission_df.index.values)\n\nprint(sum(1 for val in predicted_df.prediction if val==1))\nprint(sum(1 for val in predicted_df.prediction if val==0))\npredicted_df = predicted_df[['prediction']]\npredicted_df.to_csv('160256U_predicted_output.csv', index=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}